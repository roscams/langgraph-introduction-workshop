{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Conditional Routing\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand conditional edges in LangGraph\n",
    "- Build graphs with dynamic routing based on state\n",
    "- Implement intent-based routing patterns\n",
    "\n",
    "## Why Conditional Routing?\n",
    "\n",
    "Real-world applications need to make decisions:\n",
    "- Route to different handlers based on user intent\n",
    "- Loop back when more information is needed\n",
    "- Skip steps when they're not necessary\n",
    "\n",
    "LangGraph's **conditional edges** enable this dynamic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Router\n",
    "\n",
    "Let's build a support bot that routes to different specialists based on the query type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state\n",
    "class SupportState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query_type: str  # Will be set by the router\n",
    "\n",
    "# Router function - determines which specialist to use\n",
    "def classify_query(state: SupportState) -> dict:\n",
    "    \"\"\"Classify the user's query into a category\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"\"\"Classify the user query into one of these categories:\n",
    "        - billing: Questions about payments, invoices, subscriptions\n",
    "        - technical: Technical issues, bugs, how-to questions\n",
    "        - general: Everything else\n",
    "        \n",
    "        Respond with ONLY the category name (billing, technical, or general).\"\"\"),\n",
    "        HumanMessage(content=last_message)\n",
    "    ])\n",
    "    \n",
    "    query_type = response.content.strip().lower()\n",
    "    return {\"query_type\": query_type}\n",
    "\n",
    "# Specialist handlers\n",
    "def billing_specialist(state: SupportState) -> dict:\n",
    "    \"\"\"Handle billing-related queries\"\"\"\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a billing specialist. Help with payment and subscription issues. Be concise.\"),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def technical_specialist(state: SupportState) -> dict:\n",
    "    \"\"\"Handle technical queries\"\"\"\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a technical support specialist. Help with bugs and technical issues. Be concise.\"),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def general_specialist(state: SupportState) -> dict:\n",
    "    \"\"\"Handle general queries\"\"\"\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a helpful customer support agent. Be friendly and concise.\"),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The routing function - returns the name of the next node\n",
    "def route_to_specialist(state: SupportState) -> Literal[\"billing\", \"technical\", \"general\"]:\n",
    "    \"\"\"Route to the appropriate specialist based on query_type\"\"\"\n",
    "    query_type = state.get(\"query_type\", \"general\")\n",
    "    \n",
    "    if query_type == \"billing\":\n",
    "        return \"billing\"\n",
    "    elif query_type == \"technical\":\n",
    "        return \"technical\"\n",
    "    else:\n",
    "        return \"general\"\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(SupportState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"classify\", classify_query)\n",
    "builder.add_node(\"billing\", billing_specialist)\n",
    "builder.add_node(\"technical\", technical_specialist)\n",
    "builder.add_node(\"general\", general_specialist)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"classify\")\n",
    "\n",
    "# Add conditional edge from classifier\n",
    "builder.add_conditional_edges(\n",
    "    \"classify\",  # Source node\n",
    "    route_to_specialist,  # Function that decides the route\n",
    "    {  # Mapping of return values to node names\n",
    "        \"billing\": \"billing\",\n",
    "        \"technical\": \"technical\",\n",
    "        \"general\": \"general\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All specialists go to END\n",
    "builder.add_edge(\"billing\", END)\n",
    "builder.add_edge(\"technical\", END)\n",
    "builder.add_edge(\"general\", END)\n",
    "\n",
    "support_graph = builder.compile()\n",
    "\n",
    "# Visualize\n",
    "display(Image(support_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different query types\n",
    "queries = [\n",
    "    \"Why was I charged twice this month?\",\n",
    "    \"The app keeps crashing when I try to upload a file\",\n",
    "    \"What are your business hours?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = support_graph.invoke({\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"query_type\": \"\"\n",
    "    })\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Routed to: {result['query_type']}\")\n",
    "    print(f\"Response: {result['messages'][-1].content}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Looping with Conditions\n",
    "\n",
    "A powerful pattern is looping until a condition is met. Let's build a fact-checker that keeps refining until confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheckState(TypedDict):\n",
    "    claim: str\n",
    "    analysis: str\n",
    "    confidence: float\n",
    "    iterations: int\n",
    "\n",
    "def analyze_claim(state: FactCheckState) -> dict:\n",
    "    \"\"\"Analyze the claim and assess confidence\"\"\"\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"\"\"Analyze this claim for accuracy. Provide:\n",
    "        1. Your analysis\n",
    "        2. A confidence score from 0.0 to 1.0. 0.0 meaning the claim is totally inaccurate, 1.0 meaning the claim is accurate.\n",
    "        \n",
    "        Format your response as:\n",
    "        ANALYSIS: <your analysis>\n",
    "        CONFIDENCE: <score>\"\"\"),\n",
    "        HumanMessage(content=f\"Claim: {state['claim']}\\n\\nPrevious analysis: {state.get('analysis', 'None')}\")\n",
    "    ])\n",
    "    \n",
    "    content = response.content\n",
    "    \n",
    "    # Parse the response\n",
    "    analysis = content.split(\"CONFIDENCE:\")[0].replace(\"ANALYSIS:\", \"\").strip()\n",
    "    try:\n",
    "        confidence = float(content.split(\"CONFIDENCE:\")[1].strip())\n",
    "    except:\n",
    "        confidence = 0.5\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": analysis,\n",
    "        \"confidence\": confidence,\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "def should_continue(state: FactCheckState) -> Literal[\"continue\", \"done\"]:\n",
    "    \"\"\"Decide whether to continue analyzing or finish\"\"\"\n",
    "    # Stop if confident enough or max iterations reached\n",
    "    if state[\"confidence\"] >= 0.8 or state[\"iterations\"] >= 3:\n",
    "        return \"done\"\n",
    "    return \"continue\"\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(FactCheckState)\n",
    "\n",
    "builder.add_node(\"analyze\", analyze_claim)\n",
    "\n",
    "builder.add_edge(START, \"analyze\")\n",
    "\n",
    "# Conditional edge that can loop back\n",
    "builder.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"analyze\",  # Loop back!\n",
    "        \"done\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "fact_checker = builder.compile()\n",
    "\n",
    "display(Image(fact_checker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fact checker\n",
    "result = fact_checker.invoke({\n",
    "    \"claim\": \"The Great Wall of China is visible from space with the naked eye.\",\n",
    "    \"analysis\": \"\",\n",
    "    \"confidence\": 0.0,\n",
    "    \"iterations\": 0\n",
    "})\n",
    "\n",
    "print(f\"Claim: {result['claim']}\")\n",
    "print(f\"Final Analysis: {result['analysis']}\")\n",
    "print(f\"Confidence: {result['confidence']}\")\n",
    "print(f\"Iterations: {result['iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern: Using LLM for Routing Decisions\n",
    "\n",
    "Instead of hard-coded rules, you can use an LLM to make routing decisions. This is especially useful for complex, nuanced decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define structured output for routing\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Decision on how to route the user's request\"\"\"\n",
    "    route: Literal[\"search\", \"calculate\", \"chat\"] = Field(\n",
    "        description=\"The route to take: search for web queries, calculate for math, chat for conversation\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Brief explanation of why this route was chosen\")\n",
    "\n",
    "# Create a structured LLM\n",
    "structured_llm = llm.with_structured_output(RouteDecision)\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    route: str\n",
    "    reasoning: str\n",
    "\n",
    "def llm_router(state: RouterState) -> dict:\n",
    "    \"\"\"Use LLM to decide the route\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    decision = structured_llm.invoke([\n",
    "        SystemMessage(content=\"Decide how to handle this user request.\"),\n",
    "        HumanMessage(content=last_message)\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"route\": decision.route,\n",
    "        \"reasoning\": decision.reasoning\n",
    "    }\n",
    "\n",
    "def handle_search(state: RouterState) -> dict:\n",
    "    return {\"messages\": [AIMessage(content=\"[Search handler] I would search the web for: \" + state[\"messages\"][-1].content)]}\n",
    "\n",
    "def handle_calculate(state: RouterState) -> dict:\n",
    "    return {\"messages\": [AIMessage(content=\"[Calculator] I would calculate: \" + state[\"messages\"][-1].content)]}\n",
    "\n",
    "def handle_chat(state: RouterState) -> dict:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def get_route(state: RouterState) -> str:\n",
    "    return state[\"route\"]\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(RouterState)\n",
    "\n",
    "builder.add_node(\"router\", llm_router)\n",
    "builder.add_node(\"search\", handle_search)\n",
    "builder.add_node(\"calculate\", handle_calculate)\n",
    "builder.add_node(\"chat\", handle_chat)\n",
    "\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\"router\", get_route, [\"search\", \"calculate\", \"chat\"])\n",
    "builder.add_edge(\"search\", END)\n",
    "builder.add_edge(\"calculate\", END)\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "smart_router = builder.compile()\n",
    "\n",
    "display(Image(smart_router.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the smart router\n",
    "test_queries = [\n",
    "    \"What's the capital of France?\",\n",
    "    \"What is 234 * 567?\",\n",
    "    \"Hello, how are you today?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = smart_router.invoke({\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"route\": \"\",\n",
    "        \"reasoning\": \"\"\n",
    "    })\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Route: {result['route']} (Reason: {result['reasoning']})\")\n",
    "    print(f\"Response: {result['messages'][-1].content}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Build a Content Moderator\n",
    "\n",
    "Create a graph that:\n",
    "1. Takes user input\n",
    "2. Classifies it as \"safe\", \"warning\", or \"blocked\"\n",
    "3. Routes to appropriate handlers:\n",
    "   - **safe**: Respond normally\n",
    "   - **warning**: Respond with a gentle reminder about guidelines\n",
    "   - **blocked**: Refuse to respond\n",
    "\n",
    "**Bonus**: Add a loop that asks for clarification if the content is ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class ModerationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    classification: str\n",
    "\n",
    "# TODO: Define classifier node\n",
    "\n",
    "# TODO: Define handler nodes (safe_handler, warning_handler, blocked_handler)\n",
    "\n",
    "# TODO: Define routing function\n",
    "\n",
    "# TODO: Build the graph\n",
    "\n",
    "# Test\n",
    "# result = moderation_graph.invoke({...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Solution (hidden)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "class ModerationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    classification: str\n",
    "\n",
    "def classify_content(state: ModerationState) -> dict:\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"\"\"Classify this content:\n",
    "        - safe: Normal, appropriate content\n",
    "        - warning: Borderline content that needs a reminder\n",
    "        - blocked: Inappropriate content that should be refused\n",
    "        \n",
    "        Respond with ONLY: safe, warning, or blocked\"\"\"),\n",
    "        HumanMessage(content=last_message)\n",
    "    ])\n",
    "    return {\"classification\": response.content.strip().lower()}\n",
    "\n",
    "def safe_handler(state: ModerationState) -> dict:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def warning_handler(state: ModerationState) -> dict:\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"Respond helpfully but include a gentle reminder about community guidelines.\"),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def blocked_handler(state: ModerationState) -> dict:\n",
    "    return {\"messages\": [AIMessage(content=\"I'm sorry, but I can't help with that request.\")]}\n",
    "\n",
    "def route_by_classification(state: ModerationState) -> str:\n",
    "    c = state.get(\"classification\", \"safe\")\n",
    "    if c == \"warning\":\n",
    "        return \"warning\"\n",
    "    elif c == \"blocked\":\n",
    "        return \"blocked\"\n",
    "    return \"safe\"\n",
    "\n",
    "builder = StateGraph(ModerationState)\n",
    "builder.add_node(\"classify\", classify_content)\n",
    "builder.add_node(\"safe\", safe_handler)\n",
    "builder.add_node(\"warning\", warning_handler)\n",
    "builder.add_node(\"blocked\", blocked_handler)\n",
    "\n",
    "builder.add_edge(START, \"classify\")\n",
    "builder.add_conditional_edges(\"classify\", route_by_classification, [\"safe\", \"warning\", \"blocked\"])\n",
    "builder.add_edge(\"safe\", END)\n",
    "builder.add_edge(\"warning\", END)\n",
    "builder.add_edge(\"blocked\", END)\n",
    "\n",
    "moderation_graph = builder.compile()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Conditional edges** enable dynamic routing based on state\n",
    "2. **Router functions** return the name of the next node\n",
    "3. You can **loop** by pointing an edge back to a previous node\n",
    "4. **Structured output** makes LLM-based routing more reliable\n",
    "5. Always include **exit conditions** for loops to prevent infinite execution\n",
    "\n",
    "## Next: Tool Integration\n",
    "\n",
    "In the next notebook, we'll learn how to give our agents tools to interact with the world!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
